import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import json
import pickle
import lime
import lime.lime_tabular
import seaborn as sns


st.title("Dashboard Score Crédit")
st.markdown("Scoring Client et Comparaison à l'ensemble des clients")
st.sidebar.header("Feature Update")
st.sidebar.markdown('This section allows you to update the values of key features to see the impact on the score')

# Overall info
@st.cache
def load_data_unscaled():
    df = pd.read_csv('data/unscaled_data_small.csv')
    return df

@st.cache
def load_scaled_data():
    df_final = pd.read_csv('data/df_final_sample.csv')
    return df_final

@st.cache(allow_output_mutation=True)
def import_models():
    model = pickle.load(open('models/LRModel2.obj','rb'))
    kdt = pickle.load(open('models/kdtree.obj','rb'))
    return model, kdt


# Import models
#model = pickle.load(open('models/LRModel2.obj','rb'))
#kdt = pickle.load(open('models/kdtree.obj','rb'))

# Function definition
def example_ids():
    'Outputs 5 sample IDs of default and non-default clients'
    normal_sample = df_final[df_final['TARGET'] == 0]['SK_ID_CURR'].sample(5).tolist()
    for i in range(0, len(normal_sample)): 
        normal_sample[i] = int(normal_sample[i])
    st.write("Examples of clients with low default risk:")
    st.write(str(normal_sample).replace('[','').replace(']', ''))
        
    default_sample = df_final[df_final['TARGET'] == 1]['SK_ID_CURR'].sample(5).tolist()
    for i in range(0, len(default_sample)): 
        default_sample[i] = int(default_sample[i])
    st.write("Examples of clients with high default risk:")
    st.write(str(default_sample).replace('[','').replace(']', ''))

def clean_lime_output(string):
    '''nettoyage des caractères de liste en sortie de LIME as_list'''
    signes = ['=>', '<=', '<', '>']
    for signe in signes :
        if signe in string :
            signe_confirme = signe
        string = string.replace(signe, '____')
    string = string.split('____')
    if string[0][-1] == ' ':
        string[0] = string[0][:-1]

    return (string, signe_confirme)

@st.cache
def interpretation(client):
    'Visualize the interpretation of the prediction generated by the model'

    #model = pickle.load(open('models/LRModel2.obj','rb'))
    #df_final = pd.read_csv('data/df_final.csv')
    # Filter the dataset on a single ID
    X = df_final[df_final['SK_ID_CURR'] == int(client)]

    # Drop columns not used in the training of the model
    X.drop(columns = ['SK_ID_CURR', 'TARGET', 'Unnamed: 0'], inplace = True)

    # Generate dataset without unrequired columns
    df_lime = df_final.drop(columns = ['SK_ID_CURR', 'TARGET', 'Unnamed: 0'])

    # Generate explainer object
    explainer = lime.lime_tabular.LimeTabularExplainer(training_data = np.array(df_lime.sample(int(0.1*df_lime.shape[0]), random_state=20)),feature_names = df_lime.columns,training_labels = df_lime.columns.tolist(),verbose=1,random_state=20,mode='classification')

    # Generate specific instance explainer
    exp = explainer.explain_instance(data_row = X.to_numpy().ravel(), predict_fn = model.predict_proba)

    # Generate pyplot figure
    fig = exp.as_pyplot_figure()

    # Generate dataframe based on Lime output
    df_interpret = pd.DataFrame(exp.as_list())

    # Clean up the format of the data frame
    df_interpret['feature'] = df_interpret[0].apply(lambda x : clean_lime_output(x)[0][0])
    df_interpret['sign'] = df_interpret[0].apply(lambda x : clean_lime_output(x)[1])
    df_interpret['limit_value'] = df_interpret[0].apply(lambda x: clean_lime_output(x)[0][-1])
    df_interpret['difference'] = df_interpret[1]

    # Rename columns
    df_interpret = df_interpret[['feature', 'sign', 'limit_value', 'difference']]
    # Add distinction between normal and default customers
    df_interpret['contribution'] = 'normal'
    df_interpret.loc[df_interpret['difference']>=0, 'contribution'] = 'default'
    
    # Add the customer's value for each feature
    df_interpret['customer_value'] = [X[feature].mean() for feature in df_interpret['feature'].values.tolist()]
    # Global average
    df_interpret['global_average'] = [df_final[feature].mean() for feature in df_interpret['feature'].values.tolist()]
    # Non-default average
    df_interpret['non_default_average'] = [df_final[df_final['TARGET'] == 0][feature].mean() for feature in df_interpret['feature'].values.tolist()]
    # Average of defaulting clients
    df_interpret['default_average'] = [df_final[df_final['TARGET'] == 1][feature].mean() for feature in df_interpret['feature'].values.tolist()]
    # Average of closest neighbours
    # Columns considered in determining closest neighbours
    cols = ['DAYS_BIRTH', 'AMT_INCOME_TOTAL', 'CODE_GENDER_F', 'CREDIT_TERM', 'CREDIT_INCOME_PERCENT']
    # Choose number of neighbours
    neighbours = 20
    # Get the IDs of the n closest neighbours
    dist, ind = kdt.query(np.array(X[cols]).reshape(1,-1), k = neighbours)
    nn_idx = ind[0]
    # Generate a n neighbours dataset
    nn_df = df_final[df_final.index.isin(nn_idx)]
    # Average of nearest neighbours
    df_interpret['similar_clients_average'] = [nn_df[feature].mean() for feature in df_interpret['feature'].values.tolist()]
    # Put together the top 3 features for each
    df_interpret = pd.concat([df_interpret[df_interpret['contribution'] == 'default'].head(3), df_interpret[df_interpret['contribution'] == 'normal'].head(3)], axis=0)
    return df_interpret.sort_values(by='contribution')

def display_graphs():
    'Generates a graph to help interpret the model prediction'
    # Generating 2 rows and 3 columns of graphs
    f, ax = plt.subplots(2, 3, figsize=(12,12), sharex=False)
    plt.subplots_adjust(hspace = 0.5, wspace = 0.75)

    i = 0
    j = 0
    liste_cols = ['Client', 'Average', 'Avg Non-Default', 'Avg Default','Similar Clients']
    for feature in df_interpret['feature']:

        sns.despine(ax=None, left=True, bottom=True, trim=False)
        sns.barplot(x = df_interpret[df_interpret['feature']==feature][['customer_value', 'global_average', 'non_default_average', 'default_average', 'similar_clients_average']].values[0],
                y = liste_cols,
                ax = ax[i, j])
        sns.axes_style("white")

        if len(feature) >= 18:
            chaine = feature[:18]+'\n'+feature[18:]
        else : 
            chaine = feature
        if df_interpret[df_interpret['feature']==feature]['contribution'].values[0] == 'default':
            chaine += '\n(Higher values reduce default risk)'
            ax[i,j].set_facecolor('#e3ffec') #contribue négativement
            ax[i,j].set_title(chaine, color='#017320')
        else:
            chaine += '\n(Lower values reduce default risk)'
            ax[i,j].set_facecolor('#ffe3e3')
            ax[i,j].set_title(chaine, color='#990024')


        if j == 2:
            i+=1
            j=0
        else:
            j+=1
        if i == 2:
            break;
    for ax in f.axes:
        plt.sca(ax)
        plt.xticks(rotation=45)
    if i!=2: #Cases without enough features to explain (ex : 445260)
        True
    st.pyplot()

# Print out examples of regular and default clients
df = load_data_unscaled()
df_final = load_scaled_data()
model, kdt = import_models()
example_ids()

# Specific client prediction
# Get client ID
client_id = st.text_input('ID Client')

# Get URL
url = 'https://home-credit-risk.herokuapp.com/predict'

def filter_dataset():
    df_small = df_final[df_final['SK_ID_CURR'] == int(client_id)]
    df_small.drop(columns = ['SK_ID_CURR', 'TARGET', 'Unnamed: 0'], inplace = True)
    return df_small

# Converting dataset to a JSON file containing all the features
def get_prediction():
    input_data = df_small.to_dict()
    input_data = json.dumps(input_data)
    input_data = json.loads(input_data)
    # Post JSON file
    r = requests.post(url, json = input_data)
    # Visualize response
    st.write(r.json())

def get_prediction_update():
    input_data = df_small.to_dict()
    input_data = json.dumps(input_data)
    input_data = json.loads(input_data)
    # Post JSON file
    r = requests.post(url, json = input_data)
    # Visualize response
    st.sidebar.markdown(r.json())

df_small = filter_dataset()
get_prediction()

# Visualize interpretation
with st.spinner('Loading prediction details...'):
    df_interpret = interpretation(client_id)
    st.dataframe(df_interpret)

# Display graphs
display_graphs()

# Sidebar - Value update
features_list = df_interpret['feature'].values.tolist()
features_list = tuple([''] + features_list)
ft_to_update = st.sidebar.selectbox('Which feature would you like to update', options = features_list)
if ft_to_update == '':
    default_value = 0
else:
    default_value = df_small[ft_to_update].mean()
    new_value = st.sidebar.slider(ft_to_update, min_value = df_final[ft_to_update].min(), max_value = df_final[ft_to_update].max(), value = default_value)
    if default_value == new_value:
        pass
    else:
        df_small[ft_to_update].values[0] = new_value
        get_prediction_update()

#rerun = st.sidebar.selectbox('Would you like to rerun the prediction?', options = ['YES', 'NO'], index = 1)
#if rerun == 'YES':
#    st.write('run the interpreter')
#    st.write(ft_to_update)
#    st.write(new_value)
#else:
#    pass

#feature_to_update = ''
#feature_to_update = st.sidebar.selectbox('Which feature would you like to update?', liste_features)

# Visualize highest credit requests

# Create table
credit_df = df.pivot_table('AMT_CREDIT', 'SK_ID_CURR').sort_values(by = 'AMT_CREDIT', ascending = False).head(20)
y_pos = credit_df.sort_values(by = 'AMT_CREDIT').index.astype(str)
credit_amount = credit_df.sort_values(by = 'AMT_CREDIT')['AMT_CREDIT']

# Create graph
plt.title('Clients with highest credit requests')
plt.ylabel('Client ID')
plt.xlabel('Credit Amount')
fig = plt.gcf()
fig.set_size_inches(12,8)
plt.barh(y_pos, credit_amount)

#Visualize in Streamlit
st.pyplot()

# Visualize number of people that have defaulted

# Create table
default_df = df.pivot_table('SK_ID_CURR', 'TARGET', aggfunc = 'count')
plt.bar(default_df.index.astype(str), default_df['SK_ID_CURR'])

# Create graph
plt.title('Number of clients with default vs non-defaults')
plt.ylabel('Number of clients')
plt.xlabel('Default (0 = OK, 1 = Default)')
plt.bar(default_df.index.astype(str), default_df['SK_ID_CURR'])

#Visualize in Streamlit
st.pyplot()